{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "374ceeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import holidays\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5513c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oot_folds(df, date_col='date', n_folds=5, test_window=7):\n",
    "    \"\"\"\n",
    "    Gera folds OOT com base na coluna de data, aplicável a todas as séries.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    unique_dates = sorted(df[date_col].unique())\n",
    "\n",
    "    folds = []\n",
    "    for i in range(n_folds):\n",
    "        test_end_idx = len(unique_dates) - i * test_window\n",
    "        test_start_idx = test_end_idx - test_window\n",
    "        train_end_idx = test_start_idx\n",
    "\n",
    "        if train_end_idx <= 0:\n",
    "            break\n",
    "\n",
    "        train_dates = unique_dates[:train_end_idx]\n",
    "        test_dates = unique_dates[test_start_idx:test_end_idx]\n",
    "\n",
    "        folds.append((train_dates, test_dates))\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7c7702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_metric(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0  # Evita divisão por zero\n",
    "    return np.mean(diff) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd3190",
   "metadata": {},
   "source": [
    "# Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac76b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./sample_trat_curva_D.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c9fff",
   "metadata": {},
   "source": [
    "# Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b17978",
   "metadata": {},
   "source": [
    "## changing_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17af717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "renames = {\n",
    "    \"loja_id\": \"merchant_id\",\n",
    "    'produto_id': 'product_id',\n",
    "    'data': 'date',\n",
    "    'categoria_id': 'category_id',\n",
    "    'is_medicamento': 'is_medicine',\n",
    "    'curva': 'sales_curve',\n",
    "    'estoque_final': 'ending_stock',\n",
    "    'venda': 'sales',\n",
    "    'custo': 'cost',\n",
    "    'preco': 'price',\n",
    "    'estoque_inicial': 'starting_stock',\n",
    "    'estoque_final_anterior': 'previous_ending_stock',\n",
    "    'reposicao': 'restock',\n",
    "}\n",
    "\n",
    "df = df.rename(columns=renames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780fe37",
   "metadata": {},
   "source": [
    "## droping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d5adcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    'ending_stock',\n",
    "    'starting_stock',\n",
    "    'previous_ending_stock',\n",
    "    'restock',\n",
    "    'sales_curve'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd085f1e",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51ede9",
   "metadata": {},
   "source": [
    "## Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cda5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extracting date features\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'] >= 5\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "# Brazilian holidays\n",
    "br_holidays = holidays.Brazil()\n",
    "df['is_brazilian_holiday'] = df['date'].isin(br_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75e76ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>date</th>\n",
       "      <th>category_id</th>\n",
       "      <th>is_medicine</th>\n",
       "      <th>sales</th>\n",
       "      <th>cost</th>\n",
       "      <th>price</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>is_brazilian_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19092758</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19092759</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19092760</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19092761</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19092762</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          merchant_id  product_id       date  category_id  is_medicine  sales  \\\n",
       "19092758            1           6 2022-01-01          4.0        False    0.0   \n",
       "19092759            1           6 2022-01-02          4.0        False    0.0   \n",
       "19092760            1           6 2022-01-03          4.0        False    0.0   \n",
       "19092761            1           6 2022-01-04          4.0        False    0.0   \n",
       "19092762            1           6 2022-01-05          4.0        False    0.0   \n",
       "\n",
       "              cost    price  day  month  year  day_of_week  is_weekend  \\\n",
       "19092758  0.055587  0.07722    1      1  2022            5        True   \n",
       "19092759  0.055587  0.07722    2      1  2022            6        True   \n",
       "19092760  0.055587  0.07722    3      1  2022            0       False   \n",
       "19092761  0.055587  0.07722    4      1  2022            1       False   \n",
       "19092762  0.055587  0.07722    5      1  2022            2       False   \n",
       "\n",
       "          week_of_year  is_brazilian_holiday  \n",
       "19092758            52                 False  \n",
       "19092759            52                 False  \n",
       "19092760             1                 False  \n",
       "19092761             1                 False  \n",
       "19092762             1                 False  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e34278",
   "metadata": {},
   "source": [
    "## lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "702e08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar colunas de semana anterior\n",
    "df['prev_week'] = df['week_of_year'] - 1\n",
    "df['prev_year'] = df['year']\n",
    "\n",
    "# Ajustar quando a semana for 1 (voltar para última semana do ano anterior)\n",
    "df.loc[df['week_of_year'] == 1, 'prev_week'] = 52  # ou 53 dependendo do calendário, pode ajustar se necessário\n",
    "df.loc[df['week_of_year'] == 1, 'prev_year'] = df['year'] - 1\n",
    "\n",
    "# Agrupar para calcular média de cost e price por semana\n",
    "weekly_avg = df.groupby(\n",
    "    ['product_id', 'merchant_id', 'year', 'week_of_year']\n",
    ")[['cost', 'price', 'sales']].mean().reset_index()\n",
    "\n",
    "# Merge com base na semana anterior\n",
    "df = df.merge(\n",
    "    weekly_avg,\n",
    "    left_on=['product_id', 'merchant_id', 'prev_year', 'prev_week'],\n",
    "    right_on=['product_id', 'merchant_id', 'year', 'week_of_year'],\n",
    "    how='left',\n",
    "    suffixes=('', '_prev_week')\n",
    ")\n",
    "\n",
    "# Renomear colunas de média da semana anterior\n",
    "df.rename(columns={\n",
    "    'cost_prev_week': 'prev_week_cost_avg',\n",
    "    'sales_prev_week': 'prev_week_sales_avg',\n",
    "    'price_prev_week': 'prev_week_price_avg'\n",
    "}, inplace=True)\n",
    "\n",
    "# Remover colunas auxiliares\n",
    "df = df.drop(columns=[\n",
    "        'year_prev_week', \n",
    "        'week_of_year_prev_week', \n",
    "        'prev_year', \n",
    "        'prev_week'\n",
    "    ])\n",
    "\n",
    "df = df.drop(columns=['cost', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "799c370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15440443, 16)\n",
      "(15335714, 16)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna(subset=['prev_week_cost_avg', 'prev_week_price_avg', 'prev_week_sales_avg'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340287f2",
   "metadata": {},
   "source": [
    "## Change types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51430d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "673961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_types = {\n",
    "    'category_id': 'int64',\n",
    "    'is_medicine': 'int64',\n",
    "    'is_weekend': 'int64',\n",
    "    'is_brazilian_holiday': 'int64',\n",
    "}\n",
    "\n",
    "df = df.astype(change_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a4b24f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>date</th>\n",
       "      <th>category_id</th>\n",
       "      <th>is_medicine</th>\n",
       "      <th>sales</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>is_brazilian_holiday</th>\n",
       "      <th>prev_week_cost_avg</th>\n",
       "      <th>prev_week_price_avg</th>\n",
       "      <th>prev_week_sales_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061576</td>\n",
       "      <td>0.08712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061576</td>\n",
       "      <td>0.08712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    merchant_id  product_id       date  category_id  is_medicine  sales  day  \\\n",
       "0             1           6 2022-01-01            4            0    0.0    1   \n",
       "1             1           6 2022-01-02            4            0    0.0    2   \n",
       "9             1           6 2022-01-10            4            0    0.0   10   \n",
       "10            1           6 2022-01-11            4            0    0.0   11   \n",
       "11            1           6 2022-01-12            4            0    0.0   12   \n",
       "\n",
       "    month  year  day_of_week  is_weekend  week_of_year  is_brazilian_holiday  \\\n",
       "0       1  2022            5           1            52                     0   \n",
       "1       1  2022            6           1            52                     0   \n",
       "9       1  2022            0           0             2                     0   \n",
       "10      1  2022            1           0             2                     0   \n",
       "11      1  2022            2           0             2                     0   \n",
       "\n",
       "    prev_week_cost_avg  prev_week_price_avg  prev_week_sales_avg  \n",
       "0             0.061576              0.08712                  0.0  \n",
       "1             0.061576              0.08712                  0.0  \n",
       "9             0.055587              0.07722                  0.0  \n",
       "10            0.055587              0.07722                  0.0  \n",
       "11            0.055587              0.07722                  0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02cce0",
   "metadata": {},
   "source": [
    "# Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "592a5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# split_index = int(0.8 * len(df))\n",
    "# cutoff_date = df.loc[split_index, \"date\"]\n",
    "\n",
    "# X = df[df[\"date\"] <= cutoff_date].reset_index(drop=True)\n",
    "# Y = df[df[\"date\"] > cutoff_date].reset_index(drop=True)\n",
    "\n",
    "# del df\n",
    "\n",
    "# print(f\"Tamanho do X: {len(X)}\")\n",
    "# print(f\"Tamanho do Y: {len(Y)}\")\n",
    "# print(f\"Data de corte: {cutoff_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c66af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X.drop(columns=[\"date\", \"sales\"])\n",
    "# Y_train = X[\"sales\"]\n",
    "# X_test = Y.drop(columns=[\"date\", \"sales\"])\n",
    "# Y_test = Y[\"sales\"]\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2a1d1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c87ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\n",
    "#     'Ridge': {\n",
    "#         'model': Ridge(),\n",
    "#         'params': {\n",
    "#             'alpha': [10.0, 30.0, 50.0]\n",
    "#         }\n",
    "#     },\n",
    "#     'Lasso': {\n",
    "#         'model': Lasso(),\n",
    "#         'params': {\n",
    "#             'alpha': [1.0, 3.0, 5.0, 10.0]\n",
    "#         }\n",
    "#     },\n",
    "#     'DecisionTreeRegressor': {\n",
    "#         'model': DecisionTreeRegressor(),\n",
    "#         'params': {\n",
    "#             'max_depth': [None, 10, 20, 30],\n",
    "#             'min_samples_split': [2, 5, 10],\n",
    "#             'min_samples_leaf': [1, 2, 4]\n",
    "#         }\n",
    "#     },\n",
    "#     'RandomForestRegressor': {\n",
    "#         'model': RandomForestRegressor(),\n",
    "#         'params': {\n",
    "#             'n_estimators': [20, 30]\n",
    "#         }\n",
    "#     },\n",
    "#     'XGBRegressor': {\n",
    "#         'model': xgb.XGBRegressor(),\n",
    "#         'params': {\n",
    "#             'n_estimators': [60, 100, 150, 200],\n",
    "#             'learning_rate': [0.01, 0.1, 0.5],\n",
    "#             'subsample': [0.9]\n",
    "#         }\n",
    "#     },\n",
    "#     'LGBMRegressor': {\n",
    "#         'model': LGBMRegressor(),\n",
    "#         'params': {\n",
    "#             'n_estimators': [40, 50, 100, 150, 200],\n",
    "#             'learning_rate': [0.1, 0.2, 0.3],\n",
    "#             'num_leaves': [40, 63, 70]\n",
    "#         }\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "046f1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [1.0, 5.0, 10.0] # , 30.0, 50.0\n",
    "        }\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 0.5, 1.0] #, 3.0, 5.0, 10.0\n",
    "        }\n",
    "    },\n",
    "    'LGBMRegressor': {\n",
    "        'model': LGBMRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [100],\n",
    "            'learning_rate': [0.1, 0.2, 0.3],\n",
    "            'num_leaves': [40, 80]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e6177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação final: de 2025-02-15 até 2025-02-21\n",
      "Treino + Folds: até 2025-02-14\n"
     ]
    }
   ],
   "source": [
    "# 1. Separar a última semana para validação final\n",
    "max_date = df['date'].max()\n",
    "val_start = max_date - pd.Timedelta(days=6)\n",
    "\n",
    "df_val = df[df['date'] >= val_start]\n",
    "df_train_full = df[df['date'] < val_start]\n",
    "\n",
    "print(f\"Validação final: de {df_val['date'].min().date()} até {df_val['date'].max().date()}\")\n",
    "print(f\"Treino + Folds: até {df_train_full['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab4d9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 -> Treino: 2022-01-01 até 2025-02-07 | Teste: 2025-02-08 até 2025-02-14\n",
      "Fold 2 -> Treino: 2022-01-01 até 2025-01-31 | Teste: 2025-02-01 até 2025-02-07\n",
      "Fold 3 -> Treino: 2022-01-01 até 2025-01-24 | Teste: 2025-01-25 até 2025-01-31\n",
      "Fold 4 -> Treino: 2022-01-01 até 2025-01-17 | Teste: 2025-01-18 até 2025-01-24\n",
      "Fold 5 -> Treino: 2022-01-01 até 2025-01-10 | Teste: 2025-01-11 até 2025-01-17\n"
     ]
    }
   ],
   "source": [
    "folds = generate_oot_folds(df_train_full, date_col='date', n_folds=5, test_window=7)\n",
    "\n",
    "# Vamos armazenar os conjuntos separados\n",
    "fold_data = []\n",
    "\n",
    "for i, (train_dates, test_dates) in enumerate(folds):\n",
    "    train_df = df_train_full[df_train_full['date'].isin(train_dates)]\n",
    "    test_df = df_train_full[df_train_full['date'].isin(test_dates)]\n",
    "\n",
    "    fold_data.append({\n",
    "        'fold': i + 1,\n",
    "        'train': train_df,\n",
    "        'test': test_df\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {i + 1} -> Treino: {train_df['date'].min().date()} até {train_df['date'].max().date()} | \"\n",
    "          f\"Teste: {test_df['date'].min().date()} até {test_df['date'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d59a3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running manual grid search for Ridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:15<00:00, 45.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 1.0}\n",
      "Best SMAPE (mean across folds): 164.454431\n",
      "\n",
      "\n",
      "Running manual grid search for Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:41<00:00, 93.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 1.0}\n",
      "Best SMAPE (mean across folds): 199.906211\n",
      "\n",
      "\n",
      "Running manual grid search for LGBMRegressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.293762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.753520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.622703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.593094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.597918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [02:55<14:36, 175.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.717491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.007525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.646046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.601313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.642801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [06:19<12:49, 192.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.543766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.622180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.645552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.749599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.634515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [08:57<08:49, 176.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.557705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.714889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.578743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.683669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.561990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [11:58<05:56, 178.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.546036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.841065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.970211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.792492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.853409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [19:41<04:41, 281.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.571874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 15086847, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024858\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.648849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 954\n",
      "[LightGBM] [Info] Number of data points in the train set: 14963242, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.024955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.704414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 953\n",
      "[LightGBM] [Info] Number of data points in the train set: 14840420, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025068\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.797573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 14718246, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.761857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14596888, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.025287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [23:01<00:00, 230.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'learning_rate': 0.3, 'num_leaves': 80}\n",
      "Best SMAPE (mean across folds): 195.892886\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Cria a pasta 'models' se ela não existir\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "model_best_params = {}\n",
    "best_preds = 0\n",
    "\n",
    "for model_name, config in model_params.items():\n",
    "    print(f\"Running manual grid search for {model_name}...\")\n",
    "\n",
    "    model_class = config['model']\n",
    "    param_grid = config['params']\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in tqdm(param_combinations):\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold in fold_data:\n",
    "            train = fold['train']\n",
    "            test = fold['test']\n",
    "\n",
    "            X_train = train.drop(columns=[\"date\", \"sales\"])\n",
    "            Y_train = train[\"sales\"]\n",
    "            X_test = test.drop(columns=[\"date\", \"sales\"])\n",
    "            Y_test = test[\"sales\"]\n",
    "\n",
    "            model = model_class.set_params(**params)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            preds = model.predict(X_test)\n",
    "            preds = np.maximum(preds, 0)\n",
    "\n",
    "            smape = smape_metric(Y_test, preds)\n",
    "            fold_scores.append(smape)\n",
    "\n",
    "        mean_smape = np.mean(fold_scores)\n",
    "\n",
    "        if mean_smape < best_score:\n",
    "            best_score = mean_smape\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            best_preds = preds\n",
    "\n",
    "    print(f'Best Parameters: {best_params}')\n",
    "    print(f'Best SMAPE (mean across folds): {best_score:.6f}\\n\\n')\n",
    "\n",
    "    model_best_params[model_name] = {\n",
    "        'model': model_name,\n",
    "        'best_param': best_params,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "    with open('model_best_params.json', 'w') as f:\n",
    "        json.dump(model_best_params, f, indent=4)\n",
    "\n",
    "    with open(f'./models/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    del model, best_model, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c9ba1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE na validação final (última semana): 0.0273\n"
     ]
    }
   ],
   "source": [
    "# Separa X e y\n",
    "X_train = df_train_full.drop(columns=[\"date\", \"sales\"])\n",
    "Y_train = df_train_full[\"sales\"]\n",
    "X_test = df_val.drop(columns=[\"date\", \"sales\"])\n",
    "Y_test = df_val[\"sales\"]\n",
    "\n",
    "# Ajusta modelo final\n",
    "final_model = Ridge(alpha=1.0)\n",
    "final_model.fit(X_train, Y_train)\n",
    "\n",
    "# Faz predições na validação\n",
    "val_preds = np.maximum(final_model.predict(X_test), 0)\n",
    "\n",
    "# Avalia desempenho\n",
    "val_mae = mean_absolute_error(Y_test, val_preds)\n",
    "print(f\"MAE na validação final (última semana): {val_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d51f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
